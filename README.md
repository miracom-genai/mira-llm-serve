# Miracom LLM Serve

## I. Getting Started

### I-1. Ollama ì„¤ì¹˜

> OllamaëŠ” ë¡œì»¬ì—ì„œ LLMì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” í”Œë«í¼

* https://ollama.com/download ì ‘ì†í•˜ì—¬ OS í™˜ê²½ì— ë§ëŠ” Installer ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜

![mira-llm-serve-res-01](https://github.com/miracom-genai/mira-llm-serve/assets/5626425/706428b8-28f2-4c8b-b3b8-16be8f882404)

#### [Llama 2 ì„¤ì¹˜ ë° ì‹¤í–‰]

```shell
$ ollama run llama2
>>> Hello~
Hello! It's nice to meet you. Is there something I can help you with or
would you like to chat?

>>> Send a message (/? for help)
```

### I-2. HuggingFace GGUF ë‹¤ìš´ë¡œë“œ

> ğŸ’¡ __HuggingFace:__ ë¨¸ì‹ ëŸ¬ë‹ ì»¤ë®¤ë‹ˆí‹°ë¡œ ëª¨ë¸, ë°ì´í„°ì…‹ ë° ì‘ìš© í”„ë¡œê·¸ë¨ì„ í˜‘ì—…í•˜ëŠ” í”Œë«í¼
>
> GGUF (GPT-Generated Unified Format)

* [yanolja/EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)

#### [Huggingface CLI ì„¤ì¹˜]

```shell
$ pip install huggingface_hub
```

#### [huggingface-clië¡œ GGUF Download]

```shell
$ huggingface-cli download \
> heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF \
> ggml-model-Q5_K_M.gguf \
> --local-dir ~/ollama \
> --local-dir-use-symlinks False
Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.
downloading https://huggingface.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF/resolve/main/ggml-model-Q5_K_M.gguf to /Users/julio/.cache/huggingface/hub/tmpa7nhhbgl
ggml-model-Q5_K_M.gguf:   4%|â–Œ              | 283M/7.65G [00:22<10:05, 12.2MB/s]
```

#### [GGUF Download ì‚¬ì´íŠ¸]

* [EEVE-Korean-Instruct-10.8B-v1.0-GGUF](https://huggingface.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF/blob/main/ggml-model-Q5_K_M.gguf)

### I-3. ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±

#### [Modelfile ìƒì„±]

> `ggml-model-Q5_K_M.gguf` íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•œ ìœ„ì¹˜ì— ìƒì„±

```txt
FROM ggml-model-Q5_K_M.gguf

TEMPLATE """{{- if .System }}
<s>{{ .System }}</s>
{{- end }}
<s>Human:
{{ .Prompt }}</s>
<s>Assistant:
"""

SYSTEM """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."""

PARAMETER stop <s>
PARAMETER stop </s>
```

#### [Ollama Model ëª©ë¡ í™•ì¸]

```shell
$ ollama list
NAME         	ID          	SIZE  	MODIFIED
llama2:latest	78e26419b446	3.8 GB	2 hours ago
```

#### [Ollama Model ë“±ë¡]

```shell
$ ollama create EEVE-Korean-10.8B -f ~/ollama/EEVE-Korean-Instruct-10.8B-v1.0-GGUF/Modelfile
2024/04/12 21:09:42 parser.go:73: WARN Unknown command: TEMPERATURE
transferring model data
creating model layer
creating template layer
creating system layer
creating parameters layer
creating config layer
using already created layer sha256:b9e3d1ad5e8aa6db09610d4051820f06a5257b7d7f0b06c00630e376abcfa4c1
writing layer sha256:f325fe07319c3d2716d1165b9cfa3027b1d09de996d42dbe3dc34d27df28745e
writing layer sha256:1fa69e2371b762d1882b0bd98d284f312a36c27add732016e12e52586f98a9f5
writing layer sha256:fc44d47f7d5a1b793ab68b54cdba0102140bd358739e9d78df4abf18432fb3ea
writing layer sha256:c41e9494a1e32ea2dc1ca80cc4e43a7d8bd2ff6964bbc0a811f632929aaf648d
writing manifest
success
```

#### [Ollama Model ë“±ë¡ í™•ì¸]

```shell
$ ollama list
NAME                    	ID          	SIZE  	MODIFIED
EEVE-Korean-10.8B:latest	338bfeec1cb8	7.7 GB	56 seconds ago
llama2:latest           	78e26419b446	3.8 GB	2 hours ago
```

#### [Ollama Model ì‹¤í–‰]

```shell
$ ollama run EEVE-Korean-10.8B:latest
>>> Large Language Modelì— ëŒ€í•´ì„œ í•œì¤„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”
ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í›ˆë ¨ì‹œì¼œ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ì½ê³ , ì´í•´í•˜ë©°, ì‘ë‹µí•˜ëŠ” ëŠ¥ë ¥ì„ ê°œë°œ ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì´ë“¤ì€ ìì—°ì–´ ì²˜ë¦¬, ìƒì„±, ë²ˆì—­ ë° ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```

### I-4. LangServe ì‹¤í–‰

#### [LangServe í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜]

```shell
$ pip install -r requirements.txt
```

#### [LangServe ì‹¤í–‰]

```shell
$ python app/server.py

INFO:     Started server process [3102]
INFO:     Waiting for application startup.

 __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______
|  |        /   \     |  \ |  |  /  _____|    /       ||   ____||   _  \    \   \  /   / |   ____|
|  |       /  ^  \    |   \|  | |  |  __     |   (----`|  |__   |  |_)  |    \   \/   /  |  |__
|  |      /  /_\  \   |  . `  | |  | |_ |     \   \    |   __|  |      /      \      /   |   __|
|  `----./  _____  \  |  |\   | |  |__| | .----)   |   |  |____ |  |\  \----.  \    /    |  |____
|_______/__/     \__\ |__| \__|  \______| |_______/    |_______|| _| `._____|   \__/     |_______|

LANGSERVE: Playground for chain "/llm/" is live at:
LANGSERVE:  â”‚
LANGSERVE:  â””â”€â”€> /llm/playground/
LANGSERVE:
LANGSERVE: Playground for chain "/translate/" is live at:
LANGSERVE:  â”‚
LANGSERVE:  â””â”€â”€> /translate/playground/
LANGSERVE:
LANGSERVE: Playground for chain "/prompt/" is live at:
LANGSERVE:  â”‚
LANGSERVE:  â””â”€â”€> /prompt/playground/
LANGSERVE:
LANGSERVE: Playground for chain "/chat/" is live at:
LANGSERVE:  â”‚
LANGSERVE:  â””â”€â”€> /chat/playground/
LANGSERVE:
LANGSERVE: See all available routes at /docs/

INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

#### [LangServe ì ‘ì†]

* http://localhost:8000/prompt/playground

![mira-llm-serve-res-04](https://github.com/miracom-genai/mira-llm-serve/assets/5626425/d04c1123-bc5f-4271-a477-ee81e8e0b90d)